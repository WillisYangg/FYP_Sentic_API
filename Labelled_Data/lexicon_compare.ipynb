{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config_file = '../config.ini'\n",
    "config.read(config_file)\n",
    "default = config['DEFAULT-SQLALCHEMY']\n",
    "engine = sqlalchemy.create_engine('mysql+mysqlconnector://{0}:{1}@{2}/{3}'.\n",
    "                                            format(default['DB_USER'], default['DB_PASSWORD'], \n",
    "                                                    default['DB_IP'], default['DB_DATABASE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the real reason why you be sad you be attach t...</td>\n",
       "      <td>the real reason why you be sad you be attach t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my biggest problem be overthinking everything</td>\n",
       "      <td>my biggest problem be overthinking everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the worst sadness be the sadness you have teac...</td>\n",
       "      <td>the worst sadness be the sadness you have teac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i cannot make you understand i cannot make any...</td>\n",
       "      <td>i cannot make you understand i cannot make any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i do not think anyone really understand how ti...</td>\n",
       "      <td>i do not think anyone really understand how ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3077</th>\n",
       "      <td>cough sneeze be tho worst</td>\n",
       "      <td>cough sneeze be tho worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3078</th>\n",
       "      <td>i can be your sad whore ahaha</td>\n",
       "      <td>i can be your sad whore ahaha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>bro that feel you get after you sneeze</td>\n",
       "      <td>bro that feel you get after you sneeze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>long piss be the best</td>\n",
       "      <td>long piss be the best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>dwight you ignorant slut</td>\n",
       "      <td>dwight you ignorant slut</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3082 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  \\\n",
       "0     the real reason why you be sad you be attach t...   \n",
       "1         my biggest problem be overthinking everything   \n",
       "2     the worst sadness be the sadness you have teac...   \n",
       "3     i cannot make you understand i cannot make any...   \n",
       "4     i do not think anyone really understand how ti...   \n",
       "...                                                 ...   \n",
       "3077                          cough sneeze be tho worst   \n",
       "3078                      i can be your sad whore ahaha   \n",
       "3079             bro that feel you get after you sneeze   \n",
       "3080                              long piss be the best   \n",
       "3081                           dwight you ignorant slut   \n",
       "\n",
       "                                           tweet_string  \n",
       "0     the real reason why you be sad you be attach t...  \n",
       "1         my biggest problem be overthinking everything  \n",
       "2     the worst sadness be the sadness you have teac...  \n",
       "3     i cannot make you understand i cannot make any...  \n",
       "4     i do not think anyone really understand how ti...  \n",
       "...                                                 ...  \n",
       "3077                          cough sneeze be tho worst  \n",
       "3078                      i can be your sad whore ahaha  \n",
       "3079             bro that feel you get after you sneeze  \n",
       "3080                              long piss be the best  \n",
       "3081                           dwight you ignorant slut  \n",
       "\n",
       "[3082 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_df = pd.read_csv('clean_d_tweets.csv')\n",
    "df = pd.DataFrame(labelled_df['tweet'])\n",
    "df['tweet'] = df['tweet'].astype(str)\n",
    "df['tweet_string'] = df['tweet']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[the, real, reason, why, you, be, sad, you, be...</td>\n",
       "      <td>the real reason why you be sad you be attach t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[my, biggest, problem, be, overthinking, every...</td>\n",
       "      <td>my biggest problem be overthinking everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the, worst, sadness, be, the, sadness, you, h...</td>\n",
       "      <td>the worst sadness be the sadness you have teac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[i, can, not, make, you, understand, i, can, n...</td>\n",
       "      <td>i cannot make you understand i cannot make any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, do, not, think, anyone, really, understand...</td>\n",
       "      <td>i do not think anyone really understand how ti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  [the, real, reason, why, you, be, sad, you, be...   \n",
       "1  [my, biggest, problem, be, overthinking, every...   \n",
       "2  [the, worst, sadness, be, the, sadness, you, h...   \n",
       "3  [i, can, not, make, you, understand, i, can, n...   \n",
       "4  [i, do, not, think, anyone, really, understand...   \n",
       "\n",
       "                                        tweet_string  \n",
       "0  the real reason why you be sad you be attach t...  \n",
       "1      my biggest problem be overthinking everything  \n",
       "2  the worst sadness be the sadness you have teac...  \n",
       "3  i cannot make you understand i cannot make any...  \n",
       "4  i do not think anyone really understand how ti...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_string</th>\n",
       "      <th>port_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[the, real, reason, why, you, be, sad, you, be...</td>\n",
       "      <td>the real reason why you be sad you be attach t...</td>\n",
       "      <td>[the, real, reason, whi, you, be, sad, you, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[my, biggest, problem, be, overthinking, every...</td>\n",
       "      <td>my biggest problem be overthinking everything</td>\n",
       "      <td>[my, biggest, problem, be, overthink, everyth]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the, worst, sadness, be, the, sadness, you, h...</td>\n",
       "      <td>the worst sadness be the sadness you have teac...</td>\n",
       "      <td>[the, worst, sad, be, the, sad, you, have, tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[i, can, not, make, you, understand, i, can, n...</td>\n",
       "      <td>i cannot make you understand i cannot make any...</td>\n",
       "      <td>[i, can, not, make, you, understand, i, can, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, do, not, think, anyone, really, understand...</td>\n",
       "      <td>i do not think anyone really understand how ti...</td>\n",
       "      <td>[i, do, not, think, anyon, realli, understand,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  [the, real, reason, why, you, be, sad, you, be...   \n",
       "1  [my, biggest, problem, be, overthinking, every...   \n",
       "2  [the, worst, sadness, be, the, sadness, you, h...   \n",
       "3  [i, can, not, make, you, understand, i, can, n...   \n",
       "4  [i, do, not, think, anyone, really, understand...   \n",
       "\n",
       "                                        tweet_string  \\\n",
       "0  the real reason why you be sad you be attach t...   \n",
       "1      my biggest problem be overthinking everything   \n",
       "2  the worst sadness be the sadness you have teac...   \n",
       "3  i cannot make you understand i cannot make any...   \n",
       "4  i do not think anyone really understand how ti...   \n",
       "\n",
       "                                          port_tweet  \n",
       "0  [the, real, reason, whi, you, be, sad, you, be...  \n",
       "1     [my, biggest, problem, be, overthink, everyth]  \n",
       "2  [the, worst, sad, be, the, sad, you, have, tea...  \n",
       "3  [i, can, not, make, you, understand, i, can, n...  \n",
       "4  [i, do, not, think, anyon, realli, understand,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porterstemmer = PorterStemmer()\n",
    "df['port_tweet'] = df['tweet'].apply(lambda x: [porterstemmer.stem(word) for word in x])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_string</th>\n",
       "      <th>port_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[the, real, reason, why, you, be, sad, you, be...</td>\n",
       "      <td>the real reason why you be sad you be attach t...</td>\n",
       "      <td>[real, reason, whi, sad, attach, peopl, distan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[my, biggest, problem, be, overthinking, every...</td>\n",
       "      <td>my biggest problem be overthinking everything</td>\n",
       "      <td>[biggest, problem, overthink, everyth]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the, worst, sadness, be, the, sadness, you, h...</td>\n",
       "      <td>the worst sadness be the sadness you have teac...</td>\n",
       "      <td>[worst, sad, sad, teach, hide]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[i, can, not, make, you, understand, i, can, n...</td>\n",
       "      <td>i cannot make you understand i cannot make any...</td>\n",
       "      <td>[make, understand, make, anyon, understand, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, do, not, think, anyone, really, understand...</td>\n",
       "      <td>i do not think anyone really understand how ti...</td>\n",
       "      <td>[think, anyon, realli, understand, tire, act, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  [the, real, reason, why, you, be, sad, you, be...   \n",
       "1  [my, biggest, problem, be, overthinking, every...   \n",
       "2  [the, worst, sadness, be, the, sadness, you, h...   \n",
       "3  [i, can, not, make, you, understand, i, can, n...   \n",
       "4  [i, do, not, think, anyone, really, understand...   \n",
       "\n",
       "                                        tweet_string  \\\n",
       "0  the real reason why you be sad you be attach t...   \n",
       "1      my biggest problem be overthinking everything   \n",
       "2  the worst sadness be the sadness you have teac...   \n",
       "3  i cannot make you understand i cannot make any...   \n",
       "4  i do not think anyone really understand how ti...   \n",
       "\n",
       "                                          port_tweet  \n",
       "0  [real, reason, whi, sad, attach, peopl, distan...  \n",
       "1             [biggest, problem, overthink, everyth]  \n",
       "2                     [worst, sad, sad, teach, hide]  \n",
       "3  [make, understand, make, anyon, understand, ha...  \n",
       "4  [think, anyon, realli, understand, tire, act, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "df['port_tweet'] = df['port_tweet'].apply(remove_stopwords)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score\n",
       "Neutral     3017\n",
       "Negative      38\n",
       "Positive      27\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "df['score'] = df['port_tweet'].apply(lambda x: sentiment_analyzer.polarity_scores(x)['compound'])\n",
    "\n",
    "df['score'] = np.where(df['score'] < 0, 'Negative', np.where(df['score'] == 0, 'Neutral', 'Positive'))\n",
    "df['score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentences(data,name):\n",
    "    data[name]=data[name].apply(lambda x:' '.join([i+' ' for i in x]))\n",
    "    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_sentences(df,'port_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_string</th>\n",
       "      <th>port_tweet</th>\n",
       "      <th>score</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[the, real, reason, why, you, be, sad, you, be...</td>\n",
       "      <td>the real reason why you be sad you be attach t...</td>\n",
       "      <td>real reason whi sad attach peopl distant pay a...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(real, JJ), (reason, NN), (whi, NN), (sad, JJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[my, biggest, problem, be, overthinking, every...</td>\n",
       "      <td>my biggest problem be overthinking everything</td>\n",
       "      <td>biggest problem overthink everyth</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(biggest, JJS), (problem, NN), (overthink, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the, worst, sadness, be, the, sadness, you, h...</td>\n",
       "      <td>the worst sadness be the sadness you have teac...</td>\n",
       "      <td>worst sad sad teach hide</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(worst, RB), (sad, JJ), (sad, JJ), (teach, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[i, can, not, make, you, understand, i, can, n...</td>\n",
       "      <td>i cannot make you understand i cannot make any...</td>\n",
       "      <td>make understand make anyon understand happen i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(make, NN), (understand, NN), (make, VB), (an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, do, not, think, anyone, really, understand...</td>\n",
       "      <td>i do not think anyone really understand how ti...</td>\n",
       "      <td>think anyon realli understand tire act okay al...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(think, VB), (anyon, JJ), (realli, NNS), (und...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  [the, real, reason, why, you, be, sad, you, be...   \n",
       "1  [my, biggest, problem, be, overthinking, every...   \n",
       "2  [the, worst, sadness, be, the, sadness, you, h...   \n",
       "3  [i, can, not, make, you, understand, i, can, n...   \n",
       "4  [i, do, not, think, anyone, really, understand...   \n",
       "\n",
       "                                        tweet_string  \\\n",
       "0  the real reason why you be sad you be attach t...   \n",
       "1      my biggest problem be overthinking everything   \n",
       "2  the worst sadness be the sadness you have teac...   \n",
       "3  i cannot make you understand i cannot make any...   \n",
       "4  i do not think anyone really understand how ti...   \n",
       "\n",
       "                                          port_tweet    score  \\\n",
       "0  real reason whi sad attach peopl distant pay a...  Neutral   \n",
       "1                 biggest problem overthink everyth   Neutral   \n",
       "2                          worst sad sad teach hide   Neutral   \n",
       "3  make understand make anyon understand happen i...  Neutral   \n",
       "4  think anyon realli understand tire act okay al...  Neutral   \n",
       "\n",
       "                                            pos_tags  \n",
       "0  [(real, JJ), (reason, NN), (whi, NN), (sad, JJ...  \n",
       "1  [(biggest, JJS), (problem, NN), (overthink, NN...  \n",
       "2  [(worst, RB), (sad, JJ), (sad, JJ), (teach, NN...  \n",
       "3  [(make, NN), (understand, NN), (make, VB), (an...  \n",
       "4  [(think, VB), (anyon, JJ), (realli, NNS), (und...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos=neg=obj=count=0\n",
    "\n",
    "postagging = []\n",
    "\n",
    "for review in df['port_tweet']:\n",
    "    list = word_tokenize(review)\n",
    "    postagging.append(nltk.pos_tag(list))\n",
    "\n",
    "df['pos_tags'] = postagging\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def get_sentiment(word,tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    \n",
    "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "        return []\n",
    "\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "    if not lemma:\n",
    "        return []\n",
    "    \n",
    "    synsets = wn.synsets(word, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        return []\n",
    "\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "    return [synset.name(), swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n",
    "\n",
    "    pos=neg=obj=count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      -0.12\n",
      "1      -0.50\n",
      "2      -2.12\n",
      "3       0.62\n",
      "4       0.50\n",
      "        ... \n",
      "3077   -0.50\n",
      "3078   -0.88\n",
      "3079    0.00\n",
      "3080    0.75\n",
      "3081   -0.75\n",
      "Name: senti_score, Length: 3082, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_string</th>\n",
       "      <th>port_tweet</th>\n",
       "      <th>score</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>senti_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[the, real, reason, why, you, be, sad, you, be...</td>\n",
       "      <td>the real reason why you be sad you be attach t...</td>\n",
       "      <td>real reason whi sad attach peopl distant pay a...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(real, JJ), (reason, NN), (whi, NN), (sad, JJ...</td>\n",
       "      <td>-0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[my, biggest, problem, be, overthinking, every...</td>\n",
       "      <td>my biggest problem be overthinking everything</td>\n",
       "      <td>biggest problem overthink everyth</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(biggest, JJS), (problem, NN), (overthink, NN...</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the, worst, sadness, be, the, sadness, you, h...</td>\n",
       "      <td>the worst sadness be the sadness you have teac...</td>\n",
       "      <td>worst sad sad teach hide</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(worst, RB), (sad, JJ), (sad, JJ), (teach, NN...</td>\n",
       "      <td>-2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[i, can, not, make, you, understand, i, can, n...</td>\n",
       "      <td>i cannot make you understand i cannot make any...</td>\n",
       "      <td>make understand make anyon understand happen i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(make, NN), (understand, NN), (make, VB), (an...</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, do, not, think, anyone, really, understand...</td>\n",
       "      <td>i do not think anyone really understand how ti...</td>\n",
       "      <td>think anyon realli understand tire act okay al...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(think, VB), (anyon, JJ), (realli, NNS), (und...</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  [the, real, reason, why, you, be, sad, you, be...   \n",
       "1  [my, biggest, problem, be, overthinking, every...   \n",
       "2  [the, worst, sadness, be, the, sadness, you, h...   \n",
       "3  [i, can, not, make, you, understand, i, can, n...   \n",
       "4  [i, do, not, think, anyone, really, understand...   \n",
       "\n",
       "                                        tweet_string  \\\n",
       "0  the real reason why you be sad you be attach t...   \n",
       "1      my biggest problem be overthinking everything   \n",
       "2  the worst sadness be the sadness you have teac...   \n",
       "3  i cannot make you understand i cannot make any...   \n",
       "4  i do not think anyone really understand how ti...   \n",
       "\n",
       "                                          port_tweet    score  \\\n",
       "0  real reason whi sad attach peopl distant pay a...  Neutral   \n",
       "1                 biggest problem overthink everyth   Neutral   \n",
       "2                          worst sad sad teach hide   Neutral   \n",
       "3  make understand make anyon understand happen i...  Neutral   \n",
       "4  think anyon realli understand tire act okay al...  Neutral   \n",
       "\n",
       "                                            pos_tags  senti_score  \n",
       "0  [(real, JJ), (reason, NN), (whi, NN), (sad, JJ...        -0.12  \n",
       "1  [(biggest, JJS), (problem, NN), (overthink, NN...        -0.50  \n",
       "2  [(worst, RB), (sad, JJ), (sad, JJ), (teach, NN...        -2.12  \n",
       "3  [(make, NN), (understand, NN), (make, VB), (an...         0.62  \n",
       "4  [(think, VB), (anyon, JJ), (realli, NNS), (und...         0.50  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_score = []\n",
    "\n",
    "for pos_val in df['pos_tags']:\n",
    "    senti_val = [get_sentiment(x,y) for (x,y) in pos_val]\n",
    "    for score in senti_val:\n",
    "        try:\n",
    "            pos = pos + score[1]  \n",
    "            neg = neg + score[2]  \n",
    "        except:\n",
    "            continue\n",
    "    senti_score.append(round((pos - neg),2))\n",
    "    pos=neg=0    \n",
    "    \n",
    "df['senti_score'] = senti_score\n",
    "print(df['senti_score'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "senti_score\n",
       "Neutral     1268\n",
       "Negative    1036\n",
       "Positive     778\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['senti_score'] = df['senti_score'].replace(0.00, 0)\n",
    "df['senti_score'] = np.where(df['senti_score'] < 0, 'Negative', np.where(df['senti_score'] == 0, 'Neutral', 'Positive'))\n",
    "df['senti_score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_string</th>\n",
       "      <th>port_tweet</th>\n",
       "      <th>score</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>senti_score</th>\n",
       "      <th>TextBlob_Subjectivity</th>\n",
       "      <th>TextBlob_Polarity</th>\n",
       "      <th>TextBlob_Analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[the, real, reason, why, you, be, sad, you, be...</td>\n",
       "      <td>the real reason why you be sad you be attach t...</td>\n",
       "      <td>real reason whi sad attach peopl distant pay a...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(real, JJ), (reason, NN), (whi, NN), (sad, JJ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.403333</td>\n",
       "      <td>-0.093333</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[my, biggest, problem, be, overthinking, every...</td>\n",
       "      <td>my biggest problem be overthinking everything</td>\n",
       "      <td>biggest problem overthink everyth</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(biggest, JJS), (problem, NN), (overthink, NN...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the, worst, sadness, be, the, sadness, you, h...</td>\n",
       "      <td>the worst sadness be the sadness you have teac...</td>\n",
       "      <td>worst sad sad teach hide</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(worst, RB), (sad, JJ), (sad, JJ), (teach, NN...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[i, can, not, make, you, understand, i, can, n...</td>\n",
       "      <td>i cannot make you understand i cannot make any...</td>\n",
       "      <td>make understand make anyon understand happen i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(make, NN), (understand, NN), (make, VB), (an...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, do, not, think, anyone, really, understand...</td>\n",
       "      <td>i do not think anyone really understand how ti...</td>\n",
       "      <td>think anyon realli understand tire act okay al...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(think, VB), (anyon, JJ), (realli, NNS), (und...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3077</th>\n",
       "      <td>[cough, sneeze, be, tho, worst]</td>\n",
       "      <td>cough sneeze be tho worst</td>\n",
       "      <td>cough sneez tho worst</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(cough, NN), (sneez, NN), (tho, NN), (worst, ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3078</th>\n",
       "      <td>[i, can, be, your, sad, whore, ahaha]</td>\n",
       "      <td>i can be your sad whore ahaha</td>\n",
       "      <td>sad whore ahaha</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(sad, JJ), (whore, NN), (ahaha, NN)]</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>[bro, that, feel, you, get, after, you, sneeze]</td>\n",
       "      <td>bro that feel you get after you sneeze</td>\n",
       "      <td>bro feel get sneez</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(bro, NN), (feel, VB), (get, NN), (sneez, NN)]</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>[long, piss, be, the, best]</td>\n",
       "      <td>long piss be the best</td>\n",
       "      <td>long piss best</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(long, RB), (piss, JJ), (best, JJS)]</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>[dwight, you, ignorant, slut]</td>\n",
       "      <td>dwight you ignorant slut</td>\n",
       "      <td>dwight ignor slut</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[(dwight, JJ), (ignor, NN), (slut, NN)]</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3082 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  \\\n",
       "0     [the, real, reason, why, you, be, sad, you, be...   \n",
       "1     [my, biggest, problem, be, overthinking, every...   \n",
       "2     [the, worst, sadness, be, the, sadness, you, h...   \n",
       "3     [i, can, not, make, you, understand, i, can, n...   \n",
       "4     [i, do, not, think, anyone, really, understand...   \n",
       "...                                                 ...   \n",
       "3077                    [cough, sneeze, be, tho, worst]   \n",
       "3078              [i, can, be, your, sad, whore, ahaha]   \n",
       "3079    [bro, that, feel, you, get, after, you, sneeze]   \n",
       "3080                        [long, piss, be, the, best]   \n",
       "3081                      [dwight, you, ignorant, slut]   \n",
       "\n",
       "                                           tweet_string  \\\n",
       "0     the real reason why you be sad you be attach t...   \n",
       "1         my biggest problem be overthinking everything   \n",
       "2     the worst sadness be the sadness you have teac...   \n",
       "3     i cannot make you understand i cannot make any...   \n",
       "4     i do not think anyone really understand how ti...   \n",
       "...                                                 ...   \n",
       "3077                          cough sneeze be tho worst   \n",
       "3078                      i can be your sad whore ahaha   \n",
       "3079             bro that feel you get after you sneeze   \n",
       "3080                              long piss be the best   \n",
       "3081                           dwight you ignorant slut   \n",
       "\n",
       "                                             port_tweet    score  \\\n",
       "0     real reason whi sad attach peopl distant pay a...  Neutral   \n",
       "1                    biggest problem overthink everyth   Neutral   \n",
       "2                             worst sad sad teach hide   Neutral   \n",
       "3     make understand make anyon understand happen i...  Neutral   \n",
       "4     think anyon realli understand tire act okay al...  Neutral   \n",
       "...                                                 ...      ...   \n",
       "3077                             cough sneez tho worst   Neutral   \n",
       "3078                                   sad whore ahaha   Neutral   \n",
       "3079                                bro feel get sneez   Neutral   \n",
       "3080                                    long piss best   Neutral   \n",
       "3081                                 dwight ignor slut   Neutral   \n",
       "\n",
       "                                               pos_tags senti_score  \\\n",
       "0     [(real, JJ), (reason, NN), (whi, NN), (sad, JJ...    Negative   \n",
       "1     [(biggest, JJS), (problem, NN), (overthink, NN...    Negative   \n",
       "2     [(worst, RB), (sad, JJ), (sad, JJ), (teach, NN...    Negative   \n",
       "3     [(make, NN), (understand, NN), (make, VB), (an...    Positive   \n",
       "4     [(think, VB), (anyon, JJ), (realli, NNS), (und...    Positive   \n",
       "...                                                 ...         ...   \n",
       "3077  [(cough, NN), (sneez, NN), (tho, NN), (worst, ...    Negative   \n",
       "3078              [(sad, JJ), (whore, NN), (ahaha, NN)]    Negative   \n",
       "3079    [(bro, NN), (feel, VB), (get, NN), (sneez, NN)]     Neutral   \n",
       "3080              [(long, RB), (piss, JJ), (best, JJS)]    Positive   \n",
       "3081            [(dwight, JJ), (ignor, NN), (slut, NN)]    Negative   \n",
       "\n",
       "      TextBlob_Subjectivity  TextBlob_Polarity TextBlob_Analysis  \n",
       "0                  0.403333          -0.093333          Negative  \n",
       "1                  0.000000           0.000000           Neutral  \n",
       "2                  1.000000          -1.000000          Negative  \n",
       "3                  0.000000           0.000000           Neutral  \n",
       "4                  0.477778           0.377778          Positive  \n",
       "...                     ...                ...               ...  \n",
       "3077               1.000000          -1.000000          Negative  \n",
       "3078               1.000000          -0.500000          Negative  \n",
       "3079               0.000000           0.000000           Neutral  \n",
       "3080               0.350000           0.475000          Positive  \n",
       "3081               0.000000           0.000000           Neutral  \n",
       "\n",
       "[3082 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_analysis(df):\n",
    "    def getSubjectivity(text):\n",
    "        return TextBlob(text).sentiment.subjectivity\n",
    "    \n",
    "    def getPolarity(text):\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "\n",
    "    df['TextBlob_Subjectivity'] = df['tweet_string'].apply(getSubjectivity)\n",
    "    df['TextBlob_Polarity'] = df['tweet_string'].apply(getPolarity)\n",
    "\n",
    "    def getAnalysis(score):\n",
    "        if score < 0:\n",
    "            return 'Negative'\n",
    "        elif score == 0:\n",
    "            return 'Neutral'\n",
    "        else:\n",
    "            return 'Positive'\n",
    "        \n",
    "    df['TextBlob_Analysis'] = df['TextBlob_Polarity'].apply(getAnalysis )\n",
    "\n",
    "    return df\n",
    "\n",
    "sentiment_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob_Analysis\n",
       "Neutral     1249\n",
       "Positive     979\n",
       "Negative     854\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TextBlob_Analysis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         object\n",
       "sentiment    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df = pd.read_csv('../Reddit Data/final_dataset_2024-03-23.csv', index_col=0)\n",
    "reddit_df = reddit_df.rename(columns={'preprocessed-text-removal': 'text'})\n",
    "reddit_df['text'] = reddit_df['text'].astype('str')\n",
    "\n",
    "reddit_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_Reviews_data(data,name):\n",
    "    # Proprocessing the data\n",
    "    data[name]=data[name].str.lower()\n",
    "    # Code to remove the Hashtags from the text\n",
    "    data[name]=data[name].apply(lambda x:re.sub(r'\\B#\\S+','',x))\n",
    "    # Code to remove the links from the text\n",
    "    data[name]=data[name].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "    # Code to remove the Special characters from the text \n",
    "    data[name]=data[name].apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n",
    "    # Code to substitute the multiple spaces with single spaces\n",
    "    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n",
    "    # Code to remove all the single characters in the text\n",
    "    data[name]=data[name].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n",
    "    # Remove the twitter handlers\n",
    "    data[name]=data[name].apply(lambda x:re.sub('@[^\\s]+','',x))\n",
    "\n",
    "# Function to tokenize and remove the stopwords    \n",
    "def rem_stopwords_tokenize(data,name):\n",
    "      \n",
    "    def getting(sen):\n",
    "        example_sent = sen\n",
    "        \n",
    "        filtered_sentence = [] \n",
    "\n",
    "        stop_words = set(stopwords.words('english')) \n",
    "\n",
    "        word_tokens = word_tokenize(example_sent) \n",
    "        \n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "        \n",
    "        return filtered_sentence\n",
    "    # Using \"getting(sen)\" function to append edited sentence to data\n",
    "    x=[]\n",
    "    for i in data[name].values:\n",
    "        x.append(getting(i))\n",
    "    data[name]=x\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def Lemmatization(data,name):\n",
    "    def getting2(sen):\n",
    "        \n",
    "        example = sen\n",
    "        output_sentence =[]\n",
    "        word_tokens2 = word_tokenize(example)\n",
    "        lemmatized_output = [lemmatizer.lemmatize(w) for w in word_tokens2]\n",
    "        \n",
    "        # Remove characters which have length less than 2  \n",
    "        without_single_chr = [word for word in lemmatized_output if len(word) > 2]\n",
    "        # Remove numbers\n",
    "        cleaned_data_title = [word for word in without_single_chr if not word.isnumeric()]\n",
    "        \n",
    "        return cleaned_data_title\n",
    "    # Using \"getting2(sen)\" function to append edited sentence to data\n",
    "    x=[]\n",
    "    for i in data[name].values:\n",
    "        x.append(getting2(i))\n",
    "    data[name]=x\n",
    "\n",
    "def make_sentences(data,name):\n",
    "    data[name]=data[name].apply(lambda x:' '.join([i+' ' for i in x]))\n",
    "    # Removing double spaces if created\n",
    "    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>After_lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>therapist told trust issue</td>\n",
       "      <td>positive</td>\n",
       "      <td>therapist told trust issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>believe</td>\n",
       "      <td>neutral</td>\n",
       "      <td>believe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>think internet ruined know anymore</td>\n",
       "      <td>negative</td>\n",
       "      <td>think internet ruined know anymore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi longest time wondering enjoy anything anymo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>longest time wondering enjoy anything anymore ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>masturbation guilt</td>\n",
       "      <td>negative</td>\n",
       "      <td>masturbation guilt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment  \\\n",
       "0                        therapist told trust issue   positive   \n",
       "1                                           believe    neutral   \n",
       "2                think internet ruined know anymore   negative   \n",
       "3  hi longest time wondering enjoy anything anymo...  negative   \n",
       "4                                masturbation guilt   negative   \n",
       "\n",
       "                                 After_lemmatization  \n",
       "0                        therapist told trust issue   \n",
       "1                                           believe   \n",
       "2                think internet ruined know anymore   \n",
       "3  longest time wondering enjoy anything anymore ...  \n",
       "4                                masturbation guilt   "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_Reviews_data(reddit_df,'text')\n",
    "rem_stopwords_tokenize(reddit_df,'text')\n",
    "make_sentences(reddit_df,'text')\n",
    "\n",
    "#Edits After Lemmatization\n",
    "final_Edit = reddit_df['text'].copy()\n",
    "reddit_df[\"After_lemmatization\"] = final_Edit\n",
    "\n",
    "# Using the Lemmatization function to lemmatize the hotel data\n",
    "Lemmatization(reddit_df,'After_lemmatization')\n",
    "# Converting all the texts back to sentences\n",
    "make_sentences(reddit_df,'After_lemmatization')\n",
    "\n",
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa</th>\n",
       "      <th>aac</th>\n",
       "      <th>aback</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abdominal</th>\n",
       "      <th>aber</th>\n",
       "      <th>abilify</th>\n",
       "      <th>...</th>\n",
       "      <th>zfpu</th>\n",
       "      <th>zinn</th>\n",
       "      <th>zofran</th>\n",
       "      <th>zoloft</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoned</th>\n",
       "      <th>zonesexist</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  aac  aback  abandon  abandoned  \\\n",
       "0                                   0    0      0        0          0   \n",
       "1                                   0    0      0        0          0   \n",
       "2                                   0    0      0        0          0   \n",
       "3                                   0    0      0        0          0   \n",
       "4                                   0    0      0        0          0   \n",
       "\n",
       "   abandonment  abdomen  abdominal  aber  abilify  ...  zfpu  zinn  zofran  \\\n",
       "0            0        0          0     0        0  ...     0     0       0   \n",
       "1            0        0          0     0        0  ...     0     0       0   \n",
       "2            0        0          0     0        0  ...     0     0       0   \n",
       "3            0        0          0     0        0  ...     0     0       0   \n",
       "4            0        0          0     0        0  ...     0     0       0   \n",
       "\n",
       "   zoloft  zombie  zone  zoned  zonesexist  zoning  zoom  \n",
       "0       0       0     0      0           0       0     0  \n",
       "1       0       0     0      0           0       0     0  \n",
       "2       0       0     0      0           0       0     0  \n",
       "3       0       0     0      0           0       0     0  \n",
       "4       0       0     0      0           0       0     0  \n",
       "\n",
       "[5 rows x 10259 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "reddit_df['reviews_text_new'] = reddit_df['After_lemmatization'].copy()\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(reddit_df['reviews_text_new'])\n",
    "df = pd.DataFrame(X.toarray(), columns = vec.get_feature_names_out())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4821)\t1\n",
      "  (0, 9055)\t1\n",
      "  (0, 9227)\t1\n",
      "  (0, 9397)\t1\n",
      "  (1, 868)\t1\n",
      "  (2, 428)\t1\n",
      "  (2, 4726)\t1\n",
      "  (2, 5000)\t1\n",
      "  (2, 7662)\t1\n",
      "  (2, 9083)\t1\n",
      "  (3, 123)\t1\n",
      "  (3, 160)\t1\n",
      "  (3, 209)\t2\n",
      "  (3, 221)\t1\n",
      "  (3, 310)\t1\n",
      "  (3, 428)\t3\n",
      "  (3, 431)\t1\n",
      "  (3, 705)\t5\n",
      "  (3, 835)\t1\n",
      "  (3, 869)\t1\n",
      "  (3, 915)\t1\n",
      "  (3, 1331)\t1\n",
      "  (3, 1542)\t1\n",
      "  (3, 1943)\t2\n",
      "  (3, 2083)\t1\n",
      "  :\t:\n",
      "  (29525, 6482)\t1\n",
      "  (29525, 6515)\t1\n",
      "  (29525, 7152)\t1\n",
      "  (29525, 7413)\t1\n",
      "  (29525, 7428)\t1\n",
      "  (29525, 7956)\t1\n",
      "  (29525, 8044)\t1\n",
      "  (29525, 8082)\t1\n",
      "  (29525, 8125)\t1\n",
      "  (29525, 8277)\t2\n",
      "  (29525, 8332)\t1\n",
      "  (29525, 8424)\t1\n",
      "  (29525, 8612)\t1\n",
      "  (29525, 8638)\t1\n",
      "  (29525, 8709)\t1\n",
      "  (29525, 8863)\t1\n",
      "  (29525, 8927)\t2\n",
      "  (29525, 9108)\t1\n",
      "  (29525, 9675)\t1\n",
      "  (29525, 9875)\t1\n",
      "  (29525, 9943)\t1\n",
      "  (29525, 9972)\t1\n",
      "  (29525, 10114)\t1\n",
      "  (29525, 10205)\t1\n",
      "  (29525, 10252)\t1\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit(reddit_df['reviews_text_new'])\n",
    "vect.get_feature_names_out()\n",
    "# transform training data into a 'document-term matrix'\n",
    "simple_train_dtm = vect.transform(reddit_df['reviews_text_new'])\n",
    "print(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Willis\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_counts = CountVectorizer(tokenizer= word_tokenize,\n",
    "                             ngram_range=(1,3))\n",
    "\n",
    "bow_data = bow_counts.fit_transform(reddit_df['reviews_text_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data,\n",
    "                                                                    reddit_df['sentiment'],\n",
    "                                                                    test_size = 0.25,\n",
    "                                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Willis\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "noise_words = []\n",
    "tfidf_counts = TfidfVectorizer(tokenizer= word_tokenize,\n",
    "                               stop_words=noise_words,\n",
    "                               ngram_range=(1,1)) \n",
    "\n",
    "tfidf_data = tfidf_counts.fit_transform(reddit_df['reviews_text_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_data,\n",
    "                                                                            reddit_df['sentiment'],\n",
    "                                                                            test_size = 0.25,\n",
    "                                                                            random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.94      0.94      3456\n",
      "     neutral       0.92      0.99      0.95      1616\n",
      "    positive       0.93      0.89      0.91      2310\n",
      "\n",
      "    accuracy                           0.94      7382\n",
      "   macro avg       0.93      0.94      0.94      7382\n",
      "weighted avg       0.94      0.94      0.94      7382\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Willis\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "y_pred = lr.predict(X_test_bow)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_bow,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92      3456\n",
      "     neutral       0.93      0.95      0.94      1616\n",
      "    positive       0.90      0.86      0.88      2310\n",
      "\n",
      "    accuracy                           0.91      7382\n",
      "   macro avg       0.92      0.91      0.91      7382\n",
      "weighted avg       0.91      0.91      0.91      7382\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Willis\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "y_pred = lr.predict(X_test_tfidf)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_tfidf,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.88      0.86      3456\n",
      "     neutral       0.74      0.94      0.83      1616\n",
      "    positive       0.90      0.68      0.77      2310\n",
      "\n",
      "    accuracy                           0.83      7382\n",
      "   macro avg       0.83      0.83      0.82      7382\n",
      "weighted avg       0.84      0.83      0.83      7382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "y_pred = svm.predict(X_test_bow)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_bow,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94      3456\n",
      "     neutral       0.95      0.95      0.95      1616\n",
      "    positive       0.94      0.90      0.92      2310\n",
      "\n",
      "    accuracy                           0.94      7382\n",
      "   macro avg       0.94      0.93      0.94      7382\n",
      "weighted avg       0.94      0.94      0.94      7382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_tfidf,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.89      0.90      3456\n",
      "     neutral       0.93      0.98      0.95      1616\n",
      "    positive       0.85      0.83      0.84      2310\n",
      "\n",
      "    accuracy                           0.89      7382\n",
      "   macro avg       0.89      0.90      0.90      7382\n",
      "weighted avg       0.89      0.89      0.89      7382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dc = DecisionTreeClassifier()\n",
    "dc.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "y_pred = dc.predict(X_test_bow)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_bow,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.89      0.89      3456\n",
      "     neutral       0.92      0.97      0.95      1616\n",
      "    positive       0.86      0.83      0.84      2310\n",
      "\n",
      "    accuracy                           0.89      7382\n",
      "   macro avg       0.89      0.90      0.89      7382\n",
      "weighted avg       0.89      0.89      0.89      7382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dc = DecisionTreeClassifier()\n",
    "dc.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "y_pred = dc.predict(X_test_tfidf)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_tfidf,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
